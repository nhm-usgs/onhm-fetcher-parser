{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob\n",
    "import zipfile\n",
    "import rasterio\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from numpy.ma import masked\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "# import dask.dataframe as dd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# gpd.version.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gridmet(df, wght_id, wghts, data):\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            weight_id_rows = wghts.get_group(row[wght_id])\n",
    "            df.tmax.at[index] = np.nan_to_num(np_get_wval(data, weight_id_rows, row[wghts_id]) - 273.5)\n",
    "        except:\n",
    "            df.tmax.at[index] = netCDF4.default_fillvals['f8']\n",
    "            \n",
    "def map_gridmet_dask(df, wght_id, wghts, data):\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            weight_id_rows = wghts.get_group(row[wght_id])\n",
    "            tmp = np.nan_to_num(np_get_wval(data, weight_id_rows, row[wghts_id]) - 273.5)\n",
    "#             row.replace({'tmax':tmp})\n",
    "        except:\n",
    "            tmp = netCDF4.default_fillvals['f8']\n",
    "#             row.replace({'tmax':tmp})\n",
    "        df.loc[[index],['tmax']] = tmp\n",
    "            \n",
    "import netCDF4\n",
    "def np_get_wval(ndata, wghts, hru_id):\n",
    "    \"\"\"\n",
    "    Returns weighted average of ndata with weights = grp\n",
    "    1) mdata = the subset of values associated with the gridmet id's that are mapped to hru_id.\n",
    "    2) Some of these values may have nans if the gridmet id is outside of conus so only return values\n",
    "    that are inside of conus\n",
    "    3) this means that hru's that are entirely outside of conus will return nans which will ultimately,\n",
    "    outside of this function get assigned zero's.\n",
    "    4) the value is assigned the weighted average\n",
    "    :param ndata: float array of data values\n",
    "    :param wghts: float array of weights\n",
    "    :param hru_id hru id number\n",
    "    :return: numpy weighted averaged - masked to deal with nans associated with\n",
    "            ndata that is outside of the conus.\n",
    "    \"\"\"\n",
    "    mdata = np.ma.masked_array(ndata[wghts['grid_ids'].values.astype(int)],\n",
    "                               np.isnan(ndata[wghts['grid_ids'].values.astype(int)]))\n",
    "#     if np.ma.is_masked(mdata):\n",
    "#         print('returning masked value', hru_id)\n",
    "\n",
    "    # mdata = np.ma.masked_where(ndata[wghts['grid_ids'].values.astype(int)] <= 0.0,\n",
    "    #                            (ndata[wghts['grid_ids'].values.astype(int)]))\n",
    "    tmp = np.ma.average(mdata, weights=wghts['w'])\n",
    "    if tmp is masked:\n",
    "#         print('returning masked value', hru_id)\n",
    "        return netCDF4.default_fillvals['f8'] #np.nan\n",
    "\n",
    "    else:\n",
    "        return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Gridmet max temperature with geopandas and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "from pathlib import Path\n",
    "# folder = Path(r'../Data') # assumes working directory is onhm-fetcher-parser\n",
    "folder = Path(r'../Data_v1_1') # assumes working directory is onhm-fetcher-parser\n",
    "print(folder)\n",
    "# shapefiles = folder.glob(\"*_0[1-2].shp\")\n",
    "shapefiles = folder.glob(\"*2e*.shp\")\n",
    "gdf = pd.concat([\n",
    "    gpd.read_file(shp)\n",
    "    for shp in shapefiles\n",
    "]).pipe(gpd.GeoDataFrame)\n",
    "gdf.reset_index(drop=True, inplace=True)\n",
    "# gdf.plot()\n",
    "print(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy onhm hru ID (here nhru_v11 into a new simple dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Gridmet data (as netcdf file) print out some metadata\n",
    "This first bit of code follows examples from the following link:https://climate.northwestknowledge.net/MACA/OPENDAP.php\n",
    "First we open the data set and inspect the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "import json\n",
    "# delete existing file if it exists\n",
    "gmfile = Path('../Data_v1_1/test_gm4.nc')\n",
    "exists = gmfile.exists()\n",
    "if exists:\n",
    "    os.remove(gmfile)\n",
    "    print('removed existing file')\n",
    "\n",
    "url2 = 'http://thredds.northwestknowledge.net:8080/thredds/ncss/agg_met_tmmx_1979_CurrentYear_CONUS.nc'\n",
    "payload2={'var': 'daily_maximum_temperature',\n",
    "        'disableLLSubset': 'on',\n",
    "        'disableProjSubset': 'on',\n",
    "        'horizStride': '1',\n",
    "        'time_start': '2018-01-01T00:00:00Z',\n",
    "        'time_end': '2018-01-10T00:00:00Z',\n",
    "        'timeStride': '1',\n",
    "        'accept': 'netcdf'}\n",
    "\n",
    "try:\n",
    "    myfile = requests.get(url2, params=payload2)\n",
    "    myfile.raise_for_status()\n",
    "except HTTPError as http_err:\n",
    "    print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
    "except Exception as err:\n",
    "    print(f'Other error occurred: {err}')  # Python 3.6\n",
    "else:\n",
    "    print('Success!')\n",
    "    print(myfile.url)\n",
    "        \n",
    "with open(gmfile, 'wb') as fh:\n",
    "    fh.write(myfile.content)\n",
    "    fh.close()\n",
    "\n",
    "ds = xr.open_dataset(gmfile)\n",
    "# print(ds)\n",
    "\n",
    "print('\\n The meta data is: \\n', json.dumps(ds.attrs, indent=4))\n",
    "lathandle=ds['lat']\n",
    "lonhandle=ds['lon']\n",
    "timehandle=ds['day']\n",
    "\n",
    "datahandle=ds['daily_maximum_temperature'] # for aggragated download\n",
    "\n",
    "#collect data to describe geotransform\n",
    "lonmin = float(ds.attrs['geospatial_lon_min'])\n",
    "latmax = float(ds.attrs['geospatial_lat_max'])\n",
    "lonres = float(ds.attrs['geospatial_lon_resolution'])\n",
    "latres = float(ds.attrs['geospatial_lon_resolution'])\n",
    "\n",
    "#Print some information on the data\n",
    "\n",
    "print('\\n Data attributes, sizes, and coords \\n') \n",
    "print('\\n Data sizes are: \\n', datahandle.sizes)\n",
    "print('\\n Data coords are: \\n', datahandle.coords)\n",
    "\n",
    "ts = datahandle.sizes\n",
    "print(type(ts))\n",
    "print(ts['day'])\n",
    "dayshape = ts['day']\n",
    "Lonshape = ts['lon']\n",
    "Latshape = ts['lat']\n",
    "print(dayshape, Lonshape, Latshape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 585, 1386)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2 = xr.open_dataset('../Data_v1_1/2019_gm_ppt_2020_03_12.nc')\n",
    "np.shape(ds2.precipitation_amount.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds2.precipitation_amount.values[1,:,:].flatten()\n",
    "# date = datetime(year=1979,month=1,day=1)\n",
    "# date2 = datetime(year=1900,month=1,day=1)\n",
    "# tday = date-date2\n",
    "# print(tday.days)\n",
    "# ds2.daily_maximum_temperature.sel(day=date).values[:,:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28854"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = dt.date(year=1979,month=1,day=1)\n",
    "d2 = dt.date(year=1900,month=1,day=1)\n",
    "(d1-d2).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmap = pd.DataFrame(gdf.filter(['nhru_v11']))\n",
    "dfmap['tmax'] = 0.0\n",
    "print(type(dfmap))\n",
    "print(dfmap)\n",
    "nhm_id = dfmap.nhru_v11.values\n",
    "nhm_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19790101"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_integer(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wght_UofI = pd.read_csv('../Data_v1_1/tmp_Gridmet_weights_hru_v1_1e.csv')\n",
    "wghts_id = wght_UofI.columns[1]\n",
    "wghts = wght_UofI.groupby(wghts_id)\n",
    "ndata = datahandle.values[dayshape-1,:,:].flatten(order='K')\n",
    "# mdata = np.ma.masked_array(ndata[wghts['grid_ids'].values.astype(int)],\n",
    "#                                    np.isnan(ndata[wghts['grid_ids'].values.astype(int)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = dfmap.tmax.values\n",
    "index = dfmap[wghts_id].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "map_gridmet(dfmap, wghts_id, wghts, ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmap.tmax.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gridmet2(val, index, wght_id, wghts, data):\n",
    "    for i in range(val.shape[0]):\n",
    "        try:\n",
    "            weight_id_rows = wghts.get_group(index[i])\n",
    "            val[i] = np.nan_to_num(np_get_wval(data, weight_id_rows, index[i]) - 273.5)\n",
    "        except:\n",
    "            val[i] = netCDF4.default_fillvals['f8']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "map_gridmet2(val, index, wghts_id, wghts, ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val.min())\n",
    "print(val.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "fast_func = numba.jit(map_gridmet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -r 1\n",
    "# fast_func(val, index, wghts_id, wghts, ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gridmet3(val, index, wght_id, wghts, data):\n",
    "     \n",
    "    for i in range(val.shape[0]):\n",
    "        try:\n",
    "            weight_id_rows = wghts.get_group(index[i])\n",
    "            mdata = np.ma.masked_array(data[weight_id_rows['grid_ids'].values.astype(int)],\n",
    "                                   np.isnan(data[weight_id_rows['grid_ids'].values.astype(int)]))\n",
    "            tmp = np.ma.average(mdata, weights=wight_id_rows['w'])\n",
    "            if tmp is masked:\n",
    "                df.tmax.at[index] = netCDF4.default_fillvals['f8']\n",
    "            else:\n",
    "                df.tmax.at[index] = tmp-273.5\n",
    "#             df.tmax.at[index] = np.nan_to_num(np_get_wval2(data, weight_id_rows, row[wghts_id]) - 273.5)\n",
    "        except:\n",
    "            df.tmax.at[index] = netCDF4.default_fillvals['f8']\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "fast_func = numba.jit(map_gridmet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "fast_func(val, index, wghts_id, wghts, ndata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmap['tmax'] = 0.0\n",
    "df = dd.from_pandas(dfmap, npartitions=3)\n",
    "print(type(df))\n",
    "# for index, i in df.iterrows():\n",
    "#     if index % 10000 == 0:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df._meta)\n",
    "print(df._meta.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 1\n",
    "res = df.map_partitions(map_gridmet_dask, wghts_id, unique_hru_ids, ndata,\n",
    "                        meta=(pd.DataFrame({'nhru_v11':np.dtype(np.int64), 'tmax':np.dtype(np.float64)}, index=[]))).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmap.tmax.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
